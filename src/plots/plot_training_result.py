import json
import os

from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np

from src.compressions.OneBitSGD import OneBitSGD


def mean_of_arrays_with_padding(arr1, arr2):
    # Determine the length of the longer array
    max_len = max(len(arr1), len(arr2))

    # Pad the shorter array to the length of the longer array
    if len(arr1) < max_len:
        padding = arr2[len(arr1):]
        arr1_padded = np.concatenate((arr1, padding))
        arr2_padded = arr2
    elif len(arr2) < max_len:
        padding = arr1[len(arr2):]
        arr2_padded = np.concatenate((arr2, padding))
        arr1_padded = arr1
    else:
        print("ELSE")
        arr1_padded, arr2_padded = arr1, arr2

    # Compute the mean
    mean_arr = np.mean([arr1_padded, arr2_padded], axis=0)  # / 2
    return mean_arr


def plot_training_result(train_acc, train_loss, val_acc, val_loss, title, filename, save=False):
    epochs = len(train_acc)
    x = range(1, epochs + 1)

    train_acc = [float(x) for x in train_acc]
    val_acc = [float(x) for x in val_acc]
    train_loss = [float(x) for x in train_loss]
    val_loss = [float(x) for x in val_loss]

    fig = plt.figure(figsize=(12, 8))
    axes = fig.subplots(2, 2)

    # Plot Training Accuracy
    axes[0, 0].plot(x, train_acc, label='Training Accuracy', marker='o')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].legend()

    # Plot Training Loss
    axes[0, 1].plot(x, train_loss, label='Training Loss', marker='o', c="orange")
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()

    # Plot Validation Accuracy
    axes[1, 0].plot(x, val_acc, label='Validation Accuracy', marker='o')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Accuracy')
    axes[1, 0].legend()

    # Plot Validation Loss
    axes[1, 1].plot(x, val_loss, label='Validation Loss', marker='o', c="orange")
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Loss')
    axes[1, 1].legend()

    plt.subplots_adjust(top=0.9)
    fig.suptitle(title, fontsize=14)

    plt.tight_layout()
    if save:
        plt.savefig(f"../results/compression/{file}.pdf")
    else:
        plt.show()


if __name__ == "__main__":
    # file = "SGD_OneBitSGD_20230720-091040"
    test_acc_runs = [
        [0.22599999606609344, 0.5828999876976013, 0.6575999855995178, 0.586899995803833, 0.6136999726295471,
         0.5787000060081482, 0.6398000121116638, 0.5770000219345093, 0.6360999941825867, 0.628000020980835,
         0.5781999826431274, 0.7009999752044678, 0.6172999739646912, 0.6538000106811523, 0.6696000099182129,
         0.6087999939918518, 0.635699987411499, 0.6389999985694885, 0.6370999813079834, 0.6660000085830688,
         0.6003000140190125, 0.7339000105857849, 0.5950999855995178, 0.6800000071525574, 0.707099974155426,
         0.6062999963760376, 0.6510999798774719, 0.6880999803543091, 0.6514999866485596, 0.42260000109672546,
         0.6373999714851379, 0.6651999950408936, 0.6439999938011169, 0.7253999710083008, 0.6636000275611877,
         0.6428999900817871, 0.7210999727249146, 0.7096999883651733, 0.6656000018119812, 0.6938999891281128,
         0.6802999973297119, 0.5342000126838684, 0.5849000215530396, 0.5845999717712402, 0.7188000082969666,
         0.7168999910354614, 0.7465999722480774, 0.7282999753952026, 0.7376999855041504, 0.6876999735832214],
        [0.29510000348091125, 0.3012999892234802, 0.5777000188827515, 0.565500020980835, 0.4447000026702881,
         0.6022999882698059, 0.6402999758720398, 0.6225000023841858, 0.6402999758720398, 0.6256999969482422,
         0.6518999934196472, 0.5335999727249146, 0.6384999752044678, 0.529699981212616, 0.576200008392334,
         0.6740000247955322, 0.5982000231742859, 0.7077000141143799, 0.6262999773025513, 0.7103999853134155,
         0.5605999827384949, 0.660099983215332, 0.6714000105857849, 0.6852999925613403, 0.6618000268936157,
         0.546999990940094, 0.7035999894142151, 0.6793000102043152, 0.6894999742507935, 0.734000027179718,
         0.6929000020027161, 0.7002000212669373, 0.5889000296592712, 0.7002000212669373, 0.625, 0.6197999715805054,
         0.7422999739646912, 0.6610000133514404, 0.6762999892234802, 0.7196000218391418, 0.40799999237060547,
         0.6340000033378601, 0.6929000020027161, 0.695900022983551, 0.6952000260353088, 0.6935999989509583,
         0.6615999937057495, 0.6844000220298767, 0.7110000252723694, 0.6331999897956848]
        , [0.17350000143051147, 0.5788000226020813, 0.5720999836921692, 0.49070000648498535, 0.5371000170707703,
           0.6039000153541565, 0.6420999765396118, 0.5539000034332275, 0.538100004196167, 0.6068000197410583,
           0.6256999969482422, 0.41690000891685486, 0.6435999870300293, 0.5497000217437744, 0.5648999810218811,
           0.6542999744415283, 0.6046000123023987, 0.6383000016212463, 0.6988000273704529, 0.661300003528595,
           0.5835000276565552, 0.6866999864578247, 0.6983000040054321, 0.6510000228881836, 0.7026000022888184,
           0.6452999711036682, 0.6689000129699707, 0.6485000252723694, 0.7114999890327454, 0.6607999801635742,
           0.6773999929428101, 0.71670001745224, 0.6633999943733215, 0.6998999714851379, 0.6510000228881836,
           0.6723999977111816, 0.723800003528595, 0.6622999906539917, 0.6291999816894531, 0.7519999742507935,
           0.6923999786376953, 0.7056999802589417, 0.732699990272522, 0.6711999773979187, 0.54339998960495,
           0.6937999725341797, 0.6693000197410583, 0.7074999809265137, 0.7307000160217285, 0.7569000124931335]
        , [0.2287999987602234, 0.5236999988555908, 0.5583999752998352, 0.5968999862670898, 0.6532999873161316,
           0.5809000134468079, 0.5769000053405762, 0.6567999720573425, 0.5878999829292297, 0.6146000027656555,
           0.5823000073432922, 0.5188000202178955, 0.7064999938011169, 0.646399974822998, 0.5867000222206116,
           0.6517999768257141, 0.5576000213623047, 0.6707000136375427, 0.6460000276565552, 0.6827999949455261,
           0.574400007724762, 0.42289999127388, 0.7057999968528748, 0.5870000123977661, 0.6062999963760376,
           0.7093999981880188, 0.7294999957084656, 0.6678000092506409, 0.5590999722480774, 0.6430000066757202,
           0.7075999975204468, 0.6718999743461609, 0.7105000019073486, 0.630299985408783, 0.6848999857902527,
           0.7264999747276306, 0.71670001745224, 0.6474000215530396, 0.5672000050544739, 0.6635000109672546,
           0.6359000205993652, 0.646399974822998, 0.46970000863075256, 0.6279000043869019, 0.5989000201225281,
           0.7214000225067139, 0.6654000282287598, 0.7566999793052673, 0.7056999802589417, 0.7321000099182129]
        , [0.24719999730587006, 0.607200026512146, 0.590399980545044, 0.5740000009536743, 0.5378000140190125,
           0.6383000016212463, 0.6442999839782715, 0.6349999904632568, 0.6938999891281128, 0.6152999997138977,
           0.6370000243186951, 0.6897000074386597, 0.6629999876022339, 0.6220999956130981, 0.6873000264167786,
           0.7073000073432922, 0.6718999743461609, 0.7056000232696533, 0.6478000283241272, 0.7253999710083008,
           0.6080999970436096, 0.6855000257492065, 0.5845999717712402, 0.6351000070571899, 0.6969000101089478,
           0.6833000183105469, 0.6448000073432922, 0.5903000235557556, 0.6643999814987183, 0.6287999749183655,
           0.7077999711036682, 0.7307000160217285, 0.7339000105857849, 0.6521000266075134, 0.6969000101089478,
           0.6794999837875366, 0.6625000238418579, 0.7129999995231628, 0.6895999908447266, 0.6162999868392944,
           0.6597999930381775, 0.7124000191688538, 0.6610000133514404, 0.6365000009536743, 0.7402999997138977,
           0.6287999749183655, 0.7020000219345093, 0.54339998960495, 0.7263000011444092, 0.75]

    ]

    a = np.array(test_acc_runs[0])
    for i in range(1, len(test_acc_runs)):
        a = mean_of_arrays_with_padding(a, np.array(test_acc_runs[i]))
    # Convert to numpy array for easier calculations
    test_acc_runs = np.array(test_acc_runs)

    # Calculate mean, min, and max
    mean_test_acc = np.mean(test_acc_runs, axis=0)
    min_test_acc = np.min(test_acc_runs, axis=0)
    max_test_acc = np.max(test_acc_runs, axis=0)
    print(a - mean_test_acc)
    epochs = list(range(1, len(mean_test_acc) + 1))

    plt.figure(figsize=(10, 6))
    plt.plot(epochs, mean_test_acc, 'r', label="Average Accuracy")
    plt.plot(epochs, a, 'b', label="Mean Accuracy")
    plt.fill_between(epochs, min_test_acc, max_test_acc, color='red', alpha=0.1)  # Filling between min and max values
    plt.legend()
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title("Average Test Accuracy Curve with Uncertainty across Runs")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    for file in os.listdir("../results/compression/"):
        if "json" in file:
            file = file.replace(".json", "")
            f = open(f"../results/compression/{file}.json", "r")

            f = json.load(f)

            train_accuracy = f["train_acc"]
            training_loss = f["train_loss"]
            val_accuracy = f["val_acc"]
            vali_loss = f["val_loss"]
            title = f["strategy"]

            plot_training_result(train_accuracy, training_loss, val_accuracy, vali_loss,
                                 title, file,
                                 save=True)
